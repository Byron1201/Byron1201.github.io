<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Egocentric Reasoning with Spatio-Temporal CoT">
  <meta name="keywords" content="Egocentric Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .flex > div {
          width: calc(100% / 3 - 5px * 2 / 3);
          margin-top: 2.5px;
          margin-bottom: 2.5px;
          position: relative;
      }

      .flex {
          display: flex;
          flex-wrap: wrap;
          justify-content: space-between;
          padding: 0 5px;
      }
       @media (max-width: 720px) {
                .flex-dynamic > div {
                    width: calc(100% / 2 - 5px / 2);
                }
            }
      @media (max-width: 512px) {
                .flex-dynamic > div,
                .flex-dynamic-1 > div {
                    width: 100%;
                }

                .content {
                    padding: 0 12px;
                }

                .description {
                    padding: 0 12px 20px 12px;
                }

                .title {
                    padding: 20px 12px;
                }

                #logo {
                    right: 12px;
                    position: absolute;
                }

                .ruler-tight {
                    font-size: 6px;
                    column-gap: 0;
                }

                .title-long {
                    font-size: 16px;
                }

                .flex-compact .full-screen-button {
                    width: 100%;
                    height: 100%;
                    top: 0;
                    right: 0;
                    border-radius: 0;
                }

                table {
                    width: 100%;
                    font-size: 12px;
                }

                td {
                    padding: 10px 4px;
                }
                td:first-child {
                    padding-left: 12px;
                }
                td:last-child {
                    padding-right: 12px;
                }
            }
        @media (hover: hover) {
                .prompt-wrap:hover .prompt,
                .full-screen-wrap:hover .full-screen-button {
                    opacity: 1;
                    pointer-events: all;
                }
                #contributors a:hover {
                    background: #222;
                }
                tbody tr:hover {
                    background: #222;
                }
            }
        .prompt {
                max-width: calc(100% - 20px);
                position: absolute;
                background: rgba(0,0,0,0.5);
                color: rgba(255,255,255,0.9);
                font-size: 10px;
                backdrop-filter: blur(10px);
                bottom: 10px;
                left: 10px;
                padding: 5px 10px;
                border-radius: 15px;
                opacity: 0;
                pointer-events: none;
                transition: opacity 0.5s;
                box-sizing: border-box;
            }
  </style>
  <link rel="icon" type="image/png" href="./static/images/logo.webp">
</head>
<body>

<!--say sth-->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
            <a href="https://scholar.google.com/citations?user=sTCkd54AAAAJ&hl=zh-CN">Baoqi Pei</a><sup>1,2</sup>,</span>
              <span class="author-block">
              <a href="https://hyf015.github.io/">Yifei Huang</a><sup>2,3</sup>,</span>
              <span class="author-block">
                <a href="https://jazzcharles.github.io/">Jilan Xu</a><sup>2,4</sup>,</span>
                <span class="author-block">
              <a>Yuping He</a><sup>2,5</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=lRj3moAAAAAJ&hl=zh-CN">Guo Chen</a><sup>5</sup>,</span>
            <span class="author-block">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=XJLn4MYAAAAJ&hl=zh-CN&oi=ao">Fei Wu</a><sup>1</sup></span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN&oi=ao">Yu Qiao</a><sup>2</sup></span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=ssSfKpAAAAAJ&hl=zh-CN&oi=ao">Jiangmiao Pang</a><sup>2</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Zhejiang University, </span>
            <span class="author-block"><sup>2</sup>Shanghai AI Laboratory, </span>
            <span class="author-block"><sup>3</sup>The University of Tokyo </span>
            <span class="author-block"><sup>4</sup>Fudan University </span>
            <span class="author-block"><sup>5</sup>Nanjing University </span>
          </div>

          <!-- <div class="is-size-4 publication-authors"> -->
            <!-- <span class="author-block"><b>NeurIPS 2024</b></span> -->
          <!-- </div> -->


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. TODO -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.19462"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>    
              <!-- Code Link.  TODO-->
              <span class="link-block">
                <a href="https://github.com/aejion/AccVideo/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/aejion/AccVideo" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ðŸ¤—
                  </span>
                  <span>Model</span>
                </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-three-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="container has-text-justified is-max-desktop">
          <p>
            Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models (MLLMs), which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce <b>EgoThinker</b>, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense handâ€“object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning (RFT) to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
  </div>
     <!-- Overview. -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-three-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="container has-text-justified is-max-desktop">
          <p>
            Unlike general video reasoning, egocentric video reasoning poses unique challenges because it must infer an unobservable camera wearerâ€™s interactions and intentions. EgoThinker addresses this by curating EgoRe-5M, a large-scale egocentric reasoning dataset, and applying a two-stage supervised and reinforcement fine-tuning paradigm. This design empowers robust egocentric reasoning chat, handâ€“object grounding, and temporal grounding, making EgoThinker a promising foundation for wearable assistants and embodied AI.
          </p>
          <img src="./static/images/teaser.png">
        </div>
      </div>
    </div>
    <!--/ Overview. -->

  </div>
  <!-- Overview. -->
  <div class="columns is-centered has-text-centered">
   <div class="column is-three-fifths">
     <h2 class="title is-3">Instruction Tuning Data</h2>
     <div class="container has-text-justified is-max-desktop">
       <p>
        <b>(a)</b> To overcome this scale bottleneck in egocentric data, we develop a multi-stage filtering pipeline to mine high-quality egocentric clips from web-sourced videos, through the pipeline we get 8.7M high-quality egocentric clips, each containing rich, dynamic interactions suitable for downstream QA annotation and we combine them with existing egocentric dataset to form a collection of 13M egocentric video clips in total.
        <b>(b)</b> Based on the large-scale video clips, we build EgoRe-5M, an automatically generated QA corpus containing short-term perception, long-term causal reasoning, chain-of-thought rationales, and fine-grained grounding.
      </p>
       <img src="./static/images/dataset.png">
     </div>
   </div>
 </div>
 <!--/ Overview. -->

</section>

<section class="section">
  <div class="columns is-centered">
    <div class="column is-three-fifths">
    <h2 class="title is-centered has-text-centered">Qualitative Results</h2>
      <div class="container has-text-justified is-max-desktop">
        <p>
          To verify the hand-object grounding ability of EgoThinker, We compare our method to baseline Qwen2-VL, GPT-4o and expert model Grounding-DINO. We utilize different prompts tailored to each model and for each image, we use "chopping board", "knife", "right hand" as query for grounding.
      </p>
        <img src="./static/images/spatial.png">
      </div>

      <div class="container has-text-justified is-max-desktop">
        <p>
          Compared EgoThinker to baseline Qwen2-VL and one of the strongest MLLM Gemini2.5-Pro for temporal grounding task.
      </p>
        <img src="./static/images/temporal.png">
      </div>
    </div>
  </div> 
</section>

<section class="section" id="BibTeX">

  Â  <div class="container is-max-desktop content">
  
    <h2 class="title">BibTeX</h2>
  
  Â  Â  <pre><code>@article{zhang2025accvideo,
    title={AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset},
    author={Zhang, Haiyu and Chen, Xinyuan and Wang, Yaohui and Liu, Xihui and Wang, Yunhong and Qiao, Yu},
    journal={arXiv preprint arXiv:2503.19462},
    year={2025}
  }</code></pre>
  
   </div>
  
  </section>


<!--/TODO: insert citation. -->
<!--TODO: modeify bibtex-->



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks for Nerfie for release their project page code. We borrow their <a
              href="https://github.com/nerfies/nerfies.github.io">website code</a> to construct this website.
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
