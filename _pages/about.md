---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>

I am a third-year Ph.D. student at College of Computer Science and Technology, [Zhejiang University](https://www.zju.edu.cn/), supervised by Prof. [Fei Wu](https://scholar.google.com/citations?user=XJLn4MYAAAAJ&hl=en) and Prof [Yu Qiao](https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en), and I work closely with [Yifei Huang](https://hyf015.github.io/). Prior to this, I got my Bachelor's degree from [Beihang University](https://is.buaa.edu.cn/en/) in 2023.

My research interest includes general video understanding, egocentric vision perception and multimodal large language models. 


# üî• News
- *2025.09*: &nbsp;  Two papers [EgoThinker](https://github.com/InternRobotics/EgoThinker) and [Egoexobench](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=sTCkd54AAAAJ&sortby=pubdate&citation_for_view=sTCkd54AAAAJ:ufrVoPGSRksC) were accepted to **NIPS 2025**.
- *2025.06*: &nbsp;  Our [Vinci](https://github.com/OpenGVLab/vinci) was accepted to **IMMUT 2025**.
- *2025.06*: &nbsp;  Our [CoQo](https://link.springer.com/article/10.1007/s11263-025-02510-7) was accepted to **IJCV**.
- *2024.12*: &nbsp;  3 papers [EgoHOD](https://github.com/OpenRobotLab/EgoHOD/), [EgoExo-Gen](https://arxiv.org/abs/2504.11732) and [CG-Bench](https://github.com/CG-Bench/CG-Bench) were accepted to **ICLR 2025**.
- *2024.07*: &nbsp;  Our [EgoVideo](https://github.com/OpenGVLab/EgoVideo) won **<span style="color:red">7 championships</span>** in [EgoVis Challenge](https://egovis.github.io/) at CVPR 2024 Workshop.
- *2024.06*: &nbsp;  [Internvideo2](https://github.com/OpenGVLab/InternVideo) was accepted to **ECCV 2024**.
- *2024.01*: &nbsp;  [EgoexoLearn](https://github.com/OpenGVLab/EgoExoLearn) was accepted to **CVPR 2024**.

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NIPS 2025</div><img src='images/papers/EgoThinker.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT](https://github.com/InternRobotics/EgoThinker)

**Baoqi Pei**, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, et al.

[**[Paper]**](https://github.com/InternRobotics/EgoThinker)&nbsp;
[**[Code]**](https://github.com/InternRobotics/EgoThinker)&nbsp;
[**[Data]**](https://github.com/InternRobotics/EgoThinker)

- A framework which equips MLLMs with strong egocentric reasoning via EgoRe-5M dataset, spatio-temporal chain-of-thought supervision and a two-stage training stage.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/papers/EgoHOD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EgoHOD: Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning](https://arxiv.org/abs/2503.00986)

**Baoqi Pei**, Yifei Huang, Jilan Xu, Guo Chen, et al. ‚ÄÉ

[**[Paper]**](https://arxiv.org/abs/2503.00986)&nbsp;
[**[Code]**](https://github.com/InternRobotics/EgoHOD)&nbsp;
[**[Data]**](https://huggingface.co/datasets/Jazzcharles/EgoHOD)

- An egocentric video-language pretrained model that learns fine-grained egocentric video representations by modeling hand-object dynamics.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IMMUT 2025</div><img src='images/papers/Vinci.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vinci: A real-time embodied smart assistant based on egocentric vision-language model](https://arxiv.org/abs/2412.21080)

Yifei Huang\*, Jilan Xu\*, **Baoqi Pei\***, Lijin Yang, MingFang Zhang, Yuping He, Guo Chen, et al.

[**[Paper]**](https://arxiv.org/abs/2412.21080)&nbsp;
[**[Code]**](https://github.com/OpenGVLab/vinci)&nbsp;

- A real-time egocentric wearable assistant to assist users with daily tasks, including scene understanding, grounding, summarization, and future planning.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV</div><img src='images/papers/CoQo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CoQo: Guiding Audio-Visual Question Answering with Collective Question Reasoning](https://link.springer.com/article/10.1007/s11263-025-02510-7)

**Baoqi Pei**, Yifei Huang, Guo Chen, Jilan Xu, et al.

[**[Paper]**](https://link.springer.com/article/10.1007/s11263-025-02510-7)&nbsp;

- A multimodal model to parse AVQA task with a Question Guided Transformer and Collective Question-Answering Training strategy.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/papers/Internvideo2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Internvideo2: Scaling foundation models for multimodal video understanding](https://arxiv.org/pdf/2403.15377)

Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, **Baoqi Pei**, Rongkun Zheng, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Jilan Xu, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang

[**[Paper]**](https://arxiv.org/pdf/2403.15377)&nbsp;
[**[Code]**](https://github.com/OpenGVLab/InternVideo)&nbsp;

- A foundation model for video / text / audio understanding, achieving SOTA over several benchmarks.
</div>
</div>

- [Egoexobench: A benchmark for first-and third-person view video understanding in mllms](https://arxiv.org/abs/2507.18342), Yuping He, Yifei Huang, Guo Chen, **Baoqi Pei**, et al. **NIPS 2025**
- [Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence with Egocentric-Exocentric Vision](https://arxiv.org/pdf/2506.06253), Yuping He, Yifei Huang, Guo Chen, Lidong Lu, **Baoqi Pei**, et al. **Arxiv 2025**
- [X-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos](https://openreview.net/forum?id=8J2DrrWDKE), Jilan Xu, Yifei Huang, **Baoqi Pei**, Junlin Hou, Qingqiu Li, Guo Chen, et al. **ICLR 2025**
- [Cg-bench: Clue-grounded question answering benchmark for long video understanding](https://arxiv.org/abs/2412.12075), Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, **Baoqi Pei**, Jilan Xu, Yali Wang, Tong Lu, Limin Wang. **ICLR 2025**
- [Egovideo: Exploring egocentric foundation model and downstream adaptation](https://arxiv.org/abs/2406.18070), **Baoqi Pei**, Guo Chen, Jilan Xu, Yuping He, Yicheng Liu, et al. **EgoVis Challenge**
- [Video mamba suite: State space model as a versatile alternative for video understanding](https://arxiv.org/abs/2403.09626), Guo Chen\*, Yifei Huang\*, Jilan Xu\*, **Baoqi Pei\***, Zhe Chen, Zhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, Limin Wang. **Arxiv 2024**
- [Egoexolearn: A dataset for bridging asynchronous ego-and exo-centric view of procedural activities in real world](http://openaccess.thecvf.com/content/CVPR2024/html/Huang_EgoExoLearn_A_Dataset_for_Bridging_Asynchronous_Ego-_and_Exo-centric_View_CVPR_2024_paper.html), Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, **Baoqi Pei**, et al. **Arxiv 2024**

# üìñ Educations
- *2023.09 - Present*, Ph.D. in College of Computer Science and Technology, Zhejiang University.
- *2019.09 - 2023.06*, B.Sc. in College of Computer Science, BeiHang University.

# üéñ Honors and Awards
- Winner of the 7 tracks in the 1st EgoVis Workshop @ CVPR 2024
- Distinguished Paper Award in Egovis 2023/2024
- Outstanding Graduate Student in Zhejiang University, 2025
- Outstanding Student Scholarship in Beihang University, 2021

