---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>

I am a third-year Ph.D. student at College of Computer Science and Technology, Zhejiang University, supervised by Prof. [Fei Wu](https://scholar.google.com/citations?user=XJLn4MYAAAAJ&hl=en) and Prof [Yu Qiao](https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en), and I work closely with [Yifei Huang](https://hyf015.github.io/). Prior to this, I got my Bachelor's degree from [Beihang University](https://is.buaa.edu.cn/en/) in 2023.

My research interest includes general video understanding, egocentric vision perception and multimodal large language models. 


# üî• News
- *2025.09*: &nbsp;  Two papers [EgoThinker](https://github.com/InternRobotics/EgoThinker) and [Egoexobench](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=sTCkd54AAAAJ&sortby=pubdate&citation_for_view=sTCkd54AAAAJ:ufrVoPGSRksC) were accepted to **NIPS 2025**.
- *2025.06*: &nbsp;  Our [Vinci](https://github.com/OpenGVLab/vinci) was accepted to **IMMUT 2025**.
- *2025.06*: &nbsp;  Our [CoQo](https://link.springer.com/article/10.1007/s11263-025-02510-7) was accepted to **IJCV**.
- *2024.12*: &nbsp;  3 papers [EgoHOD](https://github.com/OpenRobotLab/EgoHOD/), [EgoExo-Gen](https://arxiv.org/abs/2504.11732) and [CG-Bench](https://github.com/CG-Bench/CG-Bench) were accepted to **ICLR 2025**.
- *2024.6*: &nbsp;  [Internvideo2](https://github.com/OpenGVLab/InternVideo) was accepted to **ECCV 2024**.
- *2024.07*: &nbsp;  Our [EgoVideo](https://github.com/OpenGVLab/EgoVideo) won **<span style="color:red">7 championships</span>** in [EgoVis Challenge](https://egovis.github.io/) at CVPR 2024 Workshop.
- *2024.01*: &nbsp;  [EgoexoLearn](https://github.com/OpenGVLab/EgoExoLearn) is accepted to **CVPR 2024**.

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NIPS 2025</div><img src='images/papers/EgoThinker.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT](https://github.com/InternRobotics/EgoThinker)

**Baoqi Pei**, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, et al.

[**[Paper]**](https://github.com/InternRobotics/EgoThinker)&nbsp;
[**[Code]**](https://github.com/InternRobotics/EgoThinker)&nbsp;
[**[Data]**](https://github.com/InternRobotics/EgoThinker)

- A framework which equips MLLMs with strong egocentric reasoning via EgoRe-5M dataset, spatio-temporal chain-of-thought supervision and a two-stage training stage.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/papers/EgoHOD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EgoHOD: Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning](https://arxiv.org/abs/2503.00986)

**Baoqi Pei**, Yifei Huang, Jilan Xu, Guo Chen, et al. ‚ÄÉ

[**[Paper]**](https://arxiv.org/abs/2503.00986)&nbsp;
[**[Code]**](https://github.com/InternRobotics/EgoHOD)&nbsp;
[**[Data]**](https://huggingface.co/datasets/Jazzcharles/EgoHOD)

- An egocentric video-language pretrained model that learns fine-grained egocentric video representations by modeling hand-object dynamics.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IMMUT 2025</div><img src='images/papers/Vinci.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vinci: A real-time embodied smart assistant based on egocentric vision-language model](https://arxiv.org/abs/2412.21080)

Yifei Huang*, Jilan Xu*, **Baoqi Pei\***, Lijin Yang*, MingFang Zhang, Yuping He, Guo Chen, et al.

[**[Paper]**](https://arxiv.org/abs/2412.21080)&nbsp;
[**[Code]**](https://github.com/OpenGVLab/vinci)&nbsp;

- A real-time egocentric wearable assistant to assist users with daily tasks, including scene understanding, grounding, summarization, and future planning.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV</div><img src='images/papers/CoQo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CoQo: Guiding Audio-Visual Question Answering with Collective Question Reasoning](https://link.springer.com/article/10.1007/s11263-025-02510-7)

**Baoqi Pei**, Yifei Huang, Guo Chen, etal.

[**[Paper]**](https://link.springer.com/article/10.1007/s11263-025-02510-7)&nbsp;

- A multimodal model to parse AVQA task with a Question Guided Transformer and Collective Question-Answering Training strategy.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/papers/Internvideo2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Internvideo2: Scaling foundation models for multimodal video understanding](https://arxiv.org/pdf/2403.15377)

Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, **Baoqi Pei**, Rongkun Zheng, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Jilan Xu, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang

[**[Paper]**](https://arxiv.org/pdf/2403.15377)&nbsp;
[**[Code]**](https://github.com/OpenGVLab/InternVideo)&nbsp;

- A foundation model for video / text / audio understanding, achieving SOTA over several benchmarks.
</div>
</div>

[A](https://arxiv.org/abs/2408.13252) [SIGGRAPH 2025]

Shuai Yang, Jing Tan, **Mengchen Zhang**, Tong Wu, Yixuan Li, Gordon Wetzstein, Ziwei Liu, Dahua Lin

[**[Project]**](https://ys-imtech.github.io/projects/LayerPano3D/)&nbsp;
[**[Paper]**](https://arxiv.org/abs/2408.13252)&nbsp;
[**[Code]**](https://github.com/3DTopia/LayerPano3D)&nbsp;

<hr> 

[IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations](https://lizb6626.github.io/IDArb/) [ICLR 2025]

Zhibing Li, Tong Wu, Jing Tan, **Mengchen Zhang**, Jiaqi Wang, Dahua Lin

[**[Project]**](https://lizb6626.github.io/IDArb/)&nbsp;
[**[Paper]**](https://arxiv.org/abs/2412.12083)&nbsp;
[**[Code]**](https://github.com/Lizb6626/IDArb/)&nbsp;
[**[Data]**](https://huggingface.co/datasets/lizb6626/Arb-Objaverse)

<hr>
[FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models](https://arxiv.org/abs/2412.07674) [NeurIPS 2024 (Datasets and Benchmarks Track)]

Tong Wu, Yinghao Xu, Ryan Po, **Mengchen Zhang**, Guandao Yang, Jiaqi Wang, Ziwei Liu, Dahua Lin, Gordon Wetzstein

[**[Project]**](https://fiva-dataset.github.io/)&nbsp;
[**[Paper]**](https://arxiv.org/abs/2412.07674)&nbsp;
[**[Code]**](https://github.com/wutong16/FiVA)&nbsp;
[**[Data]**](https://huggingface.co/datasets/FiVA/FiVA)


<hr> 
[Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side Images](https://arxiv.org/abs/2407.06191) [Arxiv 2024]

Zhangyang Qi, Yunhan Yang, **Mengchen Zhang**, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, Hengshuang Zhao

[**[Project]**](https://tailor3d-2024.github.io/)&nbsp;
[**[Paper]**](https://arxiv.org/abs/2407.06191)&nbsp;
[**[Code]**](https://github.com/Qi-Zhangyang/Tailor3D)&nbsp;

<hr> 
[Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](https://arxiv.org/abs/2312.15011) [Arxiv 2023]

Zhangyang Qi, Ye Fang, **Mengchen Zhang**, Zeyi Sun, Tong Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, Hengshuang Zhao

[**[Project]**](https://github.com/Qi-Zhangyang/Gemini-vs-GPT4V)&nbsp;
[**[Paper]**](https://arxiv.org/abs/2312.15011)&nbsp;

<hr> 
[Image data augmentation for deep learning: A survey](https://arxiv.org/abs/2204.08610) [Arxiv 2022]

Suorong Yang, Weikang Xiao, **Mengchen Zhang**, Suhan Guo, Jian Zhao, Furao Shen

# üìñ Educations
- *2023.09 - Present*, Ph.D. in College of Computer Science and Technology, Zhejiang University.
- *2019.09 - 2023.06*, B.Sc. in School of Artificial Intelligence, Nanjing University.

# üéñ Honors and Awards
- *2022.09*, National Scholarship

#  Misc
- Graduated from Suzhou High School
- [BIng üßä](https://lizb6626.github.io/) is both my best friend and collaborator. Wish her all the best and good luck!